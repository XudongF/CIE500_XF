{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import all the necessary packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a quick example about how pytorch handle the gradient calculation, assuming the model and loss function together is $y=x^2+1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "\n",
    "# Define a function y = x^2\n",
    "y = x ** 2 + 1\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further show an example with loss function.  Assuming we want to learn $y = 2x+1$. Therefore, $2$ and $1$ is our target learnable\n",
    "paraemter. Right now let's assume the model is $y=wx+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, w is tensor([1.], requires_grad=True)\n",
      "Before optimization, b is tensor([0.], requires_grad=True)\n",
      "Gradient of w: tensor([-13.3333])\n",
      "Gradient of b: tensor([-6.])\n",
      "After optimization, w is tensor([1.1000], requires_grad=True)\n",
      "After optimization, b is tensor([0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "y = torch.tensor([3.0, 5.0, 7.0], requires_grad=False)  # We give three observations. i.e., when x = 1, y = 3; x =2, y = 5\n",
    "\n",
    "# Initialize weights with requires_grad=True so we can compute gradients\n",
    "w = torch.tensor([1.0], requires_grad=True)  # initial guess for weight\n",
    "b = torch.tensor([0.0], requires_grad=True)  # initial guess for bias\n",
    "print(f'Before optimization, w is {w}')\n",
    "print(f'Before optimization, b is {b}')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([w,b], lr = 0.1)\n",
    "y_pred = w*x+b\n",
    "optimizer.zero_grad() # we reset the optimizer here\n",
    "loss = ((y_pred - y) ** 2).mean() # calculate the loss value \n",
    "loss.backward() # as shown before, this step calculates the gradient of both w and b\n",
    "optimizer.step()   # it performs the change of w and b\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradient of w:\", w.grad)  # ∂loss/∂w\n",
    "print(\"Gradient of b:\", b.grad)  # ∂loss/∂b\n",
    "\n",
    "print(f'After optimization, w is {w}')\n",
    "print(f'After optimization, b is {b}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wrap the optimizer into a loop, it will eventually find the best w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### this is the 0 times try\n",
      "current loss is 7.829999923706055\n",
      "After optimization, w is tensor([1.2000], requires_grad=True)\n",
      "After optimization, b is tensor([0.2000], requires_grad=True)\n",
      "##### this is the 1 times try\n",
      "current loss is 6.186666011810303\n",
      "After optimization, w is tensor([1.3000], requires_grad=True)\n",
      "After optimization, b is tensor([0.3000], requires_grad=True)\n",
      "##### this is the 2 times try\n",
      "current loss is 4.736665725708008\n",
      "After optimization, w is tensor([1.4000], requires_grad=True)\n",
      "After optimization, b is tensor([0.4000], requires_grad=True)\n",
      "##### this is the 3 times try\n",
      "current loss is 3.479998826980591\n",
      "After optimization, w is tensor([1.5000], requires_grad=True)\n",
      "After optimization, b is tensor([0.5000], requires_grad=True)\n",
      "##### this is the 4 times try\n",
      "current loss is 2.41666579246521\n",
      "After optimization, w is tensor([1.6000], requires_grad=True)\n",
      "After optimization, b is tensor([0.6000], requires_grad=True)\n",
      "##### this is the 5 times try\n",
      "current loss is 1.5466665029525757\n",
      "After optimization, w is tensor([1.7000], requires_grad=True)\n",
      "After optimization, b is tensor([0.7000], requires_grad=True)\n",
      "##### this is the 6 times try\n",
      "current loss is 0.8699995875358582\n",
      "After optimization, w is tensor([1.8000], requires_grad=True)\n",
      "After optimization, b is tensor([0.8000], requires_grad=True)\n",
      "##### this is the 7 times try\n",
      "current loss is 0.3866659700870514\n",
      "After optimization, w is tensor([1.9000], requires_grad=True)\n",
      "After optimization, b is tensor([0.9000], requires_grad=True)\n",
      "##### this is the 8 times try\n",
      "current loss is 0.0966663584113121\n",
      "After optimization, w is tensor([2.0000], requires_grad=True)\n",
      "After optimization, b is tensor([1.0000], requires_grad=True)\n",
      "##### this is the 9 times try\n",
      "current loss is 3.78956116703702e-13\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 10 times try\n",
      "current loss is 0.09552498906850815\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 11 times try\n",
      "current loss is 3.420349457883276e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 12 times try\n",
      "current loss is 0.09552469849586487\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 13 times try\n",
      "current loss is 3.4205768315587193e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 14 times try\n",
      "current loss is 0.09552445262670517\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 15 times try\n",
      "current loss is 3.4220865927636623e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 16 times try\n",
      "current loss is 0.09552440792322159\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 17 times try\n",
      "current loss is 3.423597263463307e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 18 times try\n",
      "current loss is 0.09552416950464249\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 19 times try\n",
      "current loss is 3.4238246371387504e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    optimizer = torch.optim.Adam([w,b], lr = 0.1)\n",
    "    y_pred = w*x+b\n",
    "    optimizer.zero_grad() # we reset the optimizer here\n",
    "    loss = ((y_pred - y) ** 2).mean() # calculate the loss value \n",
    "    print(f\"##### this is the {i} times try\")\n",
    "    print(f\"current loss is {loss}\")\n",
    "    loss.backward() # as shown before, this step calculates the gradient of both w and b\n",
    "    optimizer.step()   # it performs the change of w and b\n",
    "    print(f'After optimization, w is {w}')\n",
    "    print(f'After optimization, b is {b}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first process some dataset. The utilized dataset is iris dataset, a famous dataset for machine learning. https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X[:5, :])\n",
    "print(y[:5])\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a common process in machine learning, which normalize all the input data into [0-1] or [-1, 1]. \n",
    "\n",
    "It has several main benefits:\n",
    "1. Faster Convergence during Training\n",
    "2. Enhanced Stability of the Optimization Process\n",
    "3. Improved Model Accuracy\n",
    "\n",
    "package sklearn provides a useful function for different types of normalization. https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables before are stored using Numpy package. As just discussed, they are not designed for Neural Network computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test different data slicing here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4160,  0.5533,  0.6745,  0.9168,  1.6438, -0.1737,  2.1285, -0.2948,\n",
      "        -0.9007,  2.2497, -0.0525, -0.7795, -1.0218, -0.9007, -1.0218,  0.5533,\n",
      "        -1.2642, -1.0218, -0.9007, -0.2948, -0.9007, -0.1737,  2.2497, -1.5065,\n",
      "         0.4322, -0.1737, -0.4160,  0.1898, -0.0525,  0.1898, -0.5372,  0.4322,\n",
      "        -0.4160, -0.5372, -1.0218,  0.6745, -1.0218, -1.0218, -0.4160,  1.0380,\n",
      "        -1.1430, -0.0525, -1.0218, -1.0218,  0.0687, -0.9007,  1.2803,  0.1898,\n",
      "         0.3110,  2.2497, -0.4160, -1.7489, -1.8700,  0.1898,  1.6438, -1.5065,\n",
      "        -0.9007, -1.7489,  0.5533,  0.5533, -1.5065,  1.1592,  0.5533, -1.3854,\n",
      "         0.3110,  0.7957,  0.4322,  1.4015,  0.6745, -0.9007,  1.2803,  0.0687,\n",
      "         0.7957, -0.1737, -0.7795,  0.3110, -1.6277,  0.9168, -0.4160, -0.6583,\n",
      "        -0.2948,  1.7650,  1.0380, -0.9007, -1.1430,  1.0380,  1.6438, -1.1430,\n",
      "         1.0380, -1.1430,  1.2803,  1.8862,  0.5533, -0.1737,  0.7957,  0.5533,\n",
      "         0.6745, -0.2948,  0.0687, -0.5372,  0.3110, -1.1430, -0.0525, -0.0525,\n",
      "         1.5227], dtype=torch.float64)\n",
      "tensor([-0.4160, -1.5132, -0.0330, -0.2624], dtype=torch.float64)\n",
      "tensor(-0.4160, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:, 0])\n",
    "print(X_train[0, :])\n",
    "print(X_train[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable can also be put into either GPU or CPU, by the following commands. Remember, a variable in CPU cannot compute\n",
    "with a variable in GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if you want to put it to gpu, use 'cuda'\n",
    "x_GPU = X_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10) # recal the w and b in our simple example, the nn.Linear itself contains all the learnable parameters of the model.\n",
    "        self.fc2 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = IrisNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #because it is a classification problem, we cannot use mean square error as the loss function. instead, we use the cross entropy. Be assure it is just a type of equation like MSE.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # we have seen this optimizer before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8783\n",
      "Epoch [20/100], Loss: 0.6024\n",
      "Epoch [30/100], Loss: 0.4291\n",
      "Epoch [40/100], Loss: 0.3350\n",
      "Epoch [50/100], Loss: 0.2822\n",
      "Epoch [60/100], Loss: 0.2415\n",
      "Epoch [70/100], Loss: 0.2064\n",
      "Epoch [80/100], Loss: 0.1759\n",
      "Epoch [90/100], Loss: 0.1509\n",
      "Epoch [100/100], Loss: 0.1309\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() #remember this training loop, it is the most standard way to train the model, i.e. adjusting the paraemters until loss value is minimal. \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# we actually can let the training process stop if the loss value is lower than a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    predicted = torch.argmax(test_outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIE500",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
